# Guidelines for contributing

## Summary

**PRs welcome!**

- **Consider starting a discussion to see if there's interest in what you want to do.**
- **Fork the repo and submit PRs from the fork.**
- **Ensure PRs pass all CI checks.**
- **Maintain test coverage at 100%.**

## Git

- _[Why use Git?](https://www.git-scm.com/about)_ Git enables creation of multiple versions of a code repository called branches, with the ability to track and undo changes in detail.
- Install Git by [downloading](https://www.git-scm.com/downloads) from the website, or with a package manager like [Homebrew](https://brew.sh/).
- [Configure Git to connect to GitHub with SSH](https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh).
- [Fork](https://docs.github.com/en/get-started/quickstart/fork-a-repo) this repo.
- Create a [branch](https://www.git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell) in your fork.
- Commit your changes with a [properly-formatted Git commit message](https://chris.beams.io/posts/git-commit/).
- Create a [pull request (PR)](https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests) to incorporate your changes into the upstream project you forked.

## Python

### Hatch

This project uses [Hatch](https://hatch.pypa.io/latest/) for dependency management and packaging.

#### Highlights

- **Automatic virtual environment management**: [Hatch automatically manages the application environment](https://hatch.pypa.io/latest/environment/).
- **Dependency resolution**: Hatch will automatically resolve any dependency version conflicts using the [`pip` dependency resolver](https://pip.pypa.io/en/stable/topics/dependency-resolution/).
- **Dependency separation**: [Hatch supports separate lists of optional dependencies in the _pyproject.toml_](https://hatch.pypa.io/latest/config/dependency/). Production installs can skip optional dependencies for speed.
- **Builds**: [Hatch has features for easily building the project into a Python package](https://hatch.pypa.io/latest/build/) and [publishing the package to PyPI](https://hatch.pypa.io/latest/publish/).

#### Installation

[Hatch can be installed with Homebrew or `pipx`](https://hatch.pypa.io/latest/install/).

**Install project with all dependencies: `hatch env create`**.

#### Key commands

```sh
# Basic usage: https://hatch.pypa.io/latest/cli/reference/
hatch env create  # create virtual environment and install dependencies
hatch env find  # show path to virtual environment
hatch env show  # show info about available virtual environments
hatch run COMMAND  # run a command within the virtual environment
hatch shell  # activate the virtual environment, like source venv/bin/activate
hatch version  # list or update version of this package
export HATCH_ENV_TYPE_VIRTUAL_PATH=.venv  # install virtualenvs into .venv
```

### Testing with pytest

- Tests are in the _tests/_ directory.
- [pytest](https://docs.pytest.org/en/latest/) features used include:
    - [capturing `stdout` with `capfd`](https://docs.pytest.org/en/latest/how-to/capture-stdout-stderr.html)
    - [fixtures](https://docs.pytest.org/en/latest/how-to/fixtures.html)
    - [monkeypatch](https://docs.pytest.org/en/latest/how-to/monkeypatch.html)
    - [parametrize](https://docs.pytest.org/en/latest/how-to/parametrize.html)
    - [temporary directories and files (`tmp_path` and `tmp_dir`)](https://docs.pytest.org/en/latest/how-to/tmpdir.html)
- [pytest plugins](https://docs.pytest.org/en/latest/how-to/plugins.html) include:
    - [pytest-mock](https://github.com/pytest-dev/pytest-mock)
- [pytest configuration](https://docs.pytest.org/en/latest/reference/customize.html) is in _pyproject.toml_.
- Run tests with pytest and [coverage.py](https://github.com/nedbat/coveragepy) with `hatch run coverage run`.
- Test coverage reports are generated by [coverage.py](https://github.com/nedbat/coveragepy). To generate test coverage reports:
    1. Run tests with `hatch run coverage run`
    2. Generate a report with `hatch run coverage report`. To see interactive HTML coverage reports, run `hatch run coverage html` instead.

#### Integration testing instructions

Integration tests will be skipped if cloud credentials are not present. Running integration tests locally will take some additional setup.

##### Make buckets on each supported cloud platform

- [AWS S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html)
- [Backblaze B2](https://help.backblaze.com/hc/en-us/articles/1260803542610-Creating-a-B2-Bucket-using-the-Web-UI)

##### Upload objects to each bucket

Upload an object to each bucket named `.env.testing`.

The file should have this content:

```sh
# .env
AWS_ACCESS_KEY_ID_EXAMPLE=AKIAIOSFODNN7EXAMPLE
AWS_SECRET_ACCESS_KEY_EXAMPLE=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLE
CSV_VARIABLE=comma,separated,value
EMPTY_VARIABLE=''
# comment
INLINE_COMMENT=no_comment  # inline comment
JSON_EXAMPLE='{"array": [1, 2, 3], "exponent": 2.99e8, "number": 123}'
PASSWORD='64w2Q$!&,,[EXAMPLE'
QUOTES_AND_WHITESPACE='text and spaces'
URI_TO_DIRECTORY='~/dev'
URI_TO_S3_BUCKET=s3://mybucket/.env
URI_TO_SQLITE_DB=sqlite:////path/to/db.sqlite
URL_EXAMPLE=https://start.duckduckgo.com/
OBJECT_STORAGE_VARIABLE='DUDE!!! This variable came from object storage!'
```

##### Generate credentials for each supported cloud platform

There are three sets of credentials needed:

1. AWS temporary credentials
2. AWS static credentials
3. Backblaze static credentials

The [object storage docs](cloud-object-storage.md) have general info on generating the static credentials.

For AWS static credentials, [create a non-admin user](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-delegated-user.html). The user will need an [IAM policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_managed-policies.html) like the following. This project doesn't do any listing or deleting at this time, so those parts can be omitted if you're going for least privilege.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": ["s3:ListBucket"],
            "Resource": ["arn:aws:s3:::<AWS_S3_BUCKET_NAME>"]
        },
        {
            "Effect": "Allow",
            "Action": ["s3:PutObject", "s3:GetObject", "s3:DeleteObject"],
            "Resource": ["arn:aws:s3:::<AWS_S3_BUCKET_NAME>/*"]
        }
    ]
}
```

After attaching the IAM policy to the non-admin user, [generate an access key](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) for the non-admin user, set up an [AWS CLI profile](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html) named `fastenv`, and configure it with the access key for the non-admin user. AWS static credentials are now ready.

AWS temporary credentials work a little differently. [Create an IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html), with a [resource-based policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html) called a "role trust policy." The role trust policy would look like this:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:root"
            },
            "Action": ["sts:AssumeRole", "sts:TagSession"]
        }
    ]
}
```

Attach the [identity-based policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_managed-policies.html) you created for the IAM user to the role as well.

The end result is that the IAM user can assume the IAM role and obtain temporary credentials. The temporary credentials have the same IAM policy as the regular access key, so tests can be [parametrized](https://docs.pytest.org/en/latest/how-to/fixtures.html#parametrizing-fixtures) accordingly.

##### Run all the tests

Once you're finally done with all that, maybe go out for a walk or something.

Then come back, and run these magic words:

```sh
# set the required input variables
AWS_IAM_ROLE_NAME="paste-here"
AWS_S3_BUCKET_HOST="paste-here"
BACKBLAZE_B2_ACCESS_KEY_FASTENV="paste-here"
# leading space to avoid storing secret key in shell history
# set `HISTCONTROL=ignoreboth` for Bash or `setopt histignorespace` for Zsh
 BACKBLAZE_B2_SECRET_KEY_FASTENV="paste-here"
BACKBLAZE_B2_BUCKET_HOST="paste-here"
BACKBLAZE_B2_BUCKET_REGION="paste-here"
CLOUDFLARE_R2_ACCESS_KEY_FASTENV="paste-here"
 CLOUDFLARE_R2_SECRET_KEY_FASTENV="paste-here"
CLOUDFLARE_R2_BUCKET_HOST="paste-here"

# get AWS account ID from STS (replace jq with other JSON parser as needed)
AWS_ACCOUNT_ID=$(aws sts get-caller-identity | jq -r .Account)

# assume the IAM role to get temporary credentials
ASSUMED_ROLE=$(
  aws sts assume-role \
  --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/$AWS_IAM_ROLE_NAME \
  --role-session-name fastenv-testing-local-aws-cli
)

# run all tests by providing the necessary input variables
AWS_IAM_ACCESS_KEY_FASTENV=$(aws configure get fastenv.aws_access_key_id) \
  AWS_IAM_SECRET_KEY_FASTENV=$(aws configure get fastenv.aws_secret_access_key) \
  AWS_IAM_ACCESS_KEY_SESSION=$(echo $ASSUMED_ROLE | jq -r .Credentials.AccessKeyId) \
  AWS_IAM_SECRET_KEY_SESSION=$(echo $ASSUMED_ROLE | jq -r .Credentials.SecretAccessKey) \
  AWS_IAM_SESSION_TOKEN=$(echo $ASSUMED_ROLE | jq -r .Credentials.SessionToken) \
  AWS_S3_BUCKET_HOST=$AWS_S3_BUCKET_HOST \
  BACKBLAZE_B2_ACCESS_KEY_FASTENV=$BACKBLAZE_B2_ACCESS_KEY_FASTENV \
  BACKBLAZE_B2_SECRET_KEY_FASTENV=$BACKBLAZE_B2_SECRET_KEY_FASTENV \
  BACKBLAZE_B2_BUCKET_HOST=$BACKBLAZE_B2_BUCKET_HOST \
  BACKBLAZE_B2_BUCKET_REGION=$BACKBLAZE_B2_BUCKET_REGION \
  CLOUDFLARE_R2_ACCESS_KEY_FASTENV=$CLOUDFLARE_R2_ACCESS_KEY_FASTENV \
  CLOUDFLARE_R2_SECRET_KEY_FASTENV=$CLOUDFLARE_R2_SECRET_KEY_FASTENV \
  CLOUDFLARE_R2_BUCKET_HOST=$CLOUDFLARE_R2_BUCKET_HOST \
  hatch run coverage run && coverage report
```

## Code quality

### Running code quality checks

Code quality checks can be run using the Hatch scripts in _pyproject.toml_.

- Check: `hatch run check`
- Format: `hatch run format`

### Code style

- Python code is formatted with [Ruff](https://docs.astral.sh/ruff/). Ruff configuration is stored in _pyproject.toml_.
- Other web code (JSON, Markdown, YAML) is formatted with [Prettier](https://prettier.io/).

### Static type checking

- To learn type annotation basics, see the [Python typing module docs](https://docs.python.org/3/library/typing.html), [Python type annotations how-to](https://docs.python.org/3/howto/annotations.html), the [Real Python type checking tutorial](https://realpython.com/python-type-checking/), and [this gist](https://gist.github.com/987bdc6263217895d4bf03d0a5ff114c).
- Type annotations are not used at runtime. The standard library `typing` module includes a `TYPE_CHECKING` constant that is `False` at runtime, but `True` when conducting static type checking prior to runtime. Type imports are included under `if TYPE_CHECKING:` conditions so that they are not imported at runtime. These conditions are ignored when calculating test coverage.
- Type annotations can be provided inline or in separate stub files. Much of the Python standard library is annotated with stubs. For example, the Python standard library [`logging.config` module uses type stubs](https://github.com/python/typeshed/blob/main/stdlib/logging/config.pyi). The typeshed types for the `logging.config` module are used solely for type-checking usage of the `logging.config` module itself. They cannot be imported and used to type annotate other modules.
- The standard library `typing` module includes a `NoReturn` type. This would seem useful for [unreachable code](https://typing.readthedocs.io/en/stable/source/unreachable.html), including functions that do not return a value, such as test functions. Unfortunately mypy reports an error when using `NoReturn`, "Implicit return in function which does not return (misc)." To avoid headaches from the opaque "misc" category of [mypy errors](https://mypy.readthedocs.io/en/stable/error_code_list.html), these functions are annotated as returning `None`.
- [Mypy](https://mypy.readthedocs.io/en/stable/) is used for type-checking. [Mypy configuration](https://mypy.readthedocs.io/en/stable/config_file.html) is included in _pyproject.toml_.
- Mypy strict mode is enabled. Strict includes `--no-explicit-reexport` (`implicit_reexport = false`), which means that objects imported into a module will not be re-exported for import into other modules. Imports can be made into explicit exports with the syntax `from module import x as x` (i.e., changing from `import logging` to `import logging as logging`), or by including imports in `__all__`. This explicit import syntax can be confusing. Another option is to apply mypy overrides to any modules that need to leverage implicit exports.

### Spell check

Spell check is performed with [CSpell](https://cspell.org/). The CSpell command is included in the Hatch script for code quality checks (`hatch run check`).

## GitHub Actions workflows

[GitHub Actions](https://github.com/features/actions) is a continuous integration/continuous deployment (CI/CD) service that runs on GitHub repos. It replaces other services like Travis CI. Actions are grouped into workflows and stored in _.github/workflows_. See [Getting the Gist of GitHub Actions](https://gist.github.com/br3ndonland/f9c753eb27381f97336aa21b8d932be6) for more info.

### GitHub Actions and AWS

#### Static credentials

As explained in the section on [generating credentials for local testing](#generate-credentials-for-each-supported-cloud-platform), a non-admin [IAM user](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html) must be created in order to allow GitHub Actions to access AWS when using [static credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html). The IAM user for this repo was created following [IAM best practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html). In AWS, there is a `GitHubActions` IAM group, with a `fastenv` IAM user (one user per repo). The `fastenv` user has an IAM policy attached specifying its permissions.

On GitHub, the `fastenv` user access key is stored in [GitHub Secrets](https://docs.github.com/en/actions/reference/encrypted-secrets).

The bucket host is stored in GitHub Secrets in the "[virtual-hosted-style](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html)" format (`<bucketname>.s3.<region>.amazonaws.com`).

#### Temporary credentials

In addition to the static access key, GitHub Actions also retrieves temporary security credentials from AWS using OpenID Connect (OIDC). See the GitHub [docs](https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect) for further info.

The OIDC infrastructure is provisioned with Terraform, using a similar approach to the example in [br3ndonland/terraform-examples](https://github.com/br3ndonland/terraform-examples).

### GitHub Actions and Backblaze B2

A [B2 application key](https://www.backblaze.com/b2/docs/application_keys.html) is stored in GitHub Secrets, along with the corresponding bucket host in "virtual-hosted-style" format (`<bucket-name>.s3.<region-name>.backblazeb2.com`).

See the [Backblaze B2 S3-compatible API docs](https://www.backblaze.com/b2/docs/s3_compatible_api.html) for further info.

### GitHub Actions and Cloudflare R2

A [Cloudflare S3 auth token](https://developers.cloudflare.com/r2/data-access/s3-api/tokens/) (access key) is stored in GitHub Secrets, along with the corresponding bucket host in "virtual-hosted-style" format (`https://<BUCKET>.<ACCOUNT_ID>.r2.cloudflarestorage.com`).

See the [Cloudflare R2 docs](https://developers.cloudflare.com/r2/) for further info.

## Maintainers

- **PRs should be merged into the default branch.** Head branches are deleted automatically after PRs are merged.
- **Branch protection is enabled.**
    - Require signed commits
    - Include administrators
    - Do not allow force pushes
    - Require status checks to pass before merging
- **To create a release:**
    - Bump the version number in `__version__` with `hatch version` and commit the changes.
        - Follow [SemVer](https://semver.org/) guidelines when choosing a version number. Note that [PEP 440](https://peps.python.org/pep-0440/) Python version specifiers and SemVer version specifiers differ, particularly with regard to specifying prereleases. Use syntax compatible with both.
        - The PEP 440 default (like `1.0.0a0`) is different from SemVer. Hatch and PyPI will use this syntax by default.
        - An alternative form of the Python prerelease syntax permitted in PEP 440 (like `1.0.0-alpha.0`) is compatible with SemVer, and this form should be used when tagging releases. As Hatch uses PEP 440 syntax by default, prerelease versions need to be written directly into `__version__`.
        - Examples of acceptable tag names: `1.0.0`, `1.0.0-alpha.0`, `1.0.0-beta.1`
    - Push the version bump and verify all CI checks pass.
    - Create an [annotated and signed Git tag](https://www.git-scm.com/book/en/v2/Git-Basics-Tagging).
        - List PRs and commits in the tag message:
            ```sh
            git log --pretty=format:"- %s (%h)" \
              "$(git describe --abbrev=0 --tags)"..HEAD
            ```
        - Omit the leading `v` (use `1.0.0` instead of `v1.0.0`)
        - Example: `git tag -a -s 1.0.0`
    - Push the tag. GitHub Actions will build and push the Python package and Docker images, and open a PR to update the changelog.
    - Squash and merge the changelog PR.

### Deployments

Documentation is built with [Material for MkDocs](https://squidfunk.github.io/mkdocs-material/), deployed to [Vercel](https://vercel.com/) using the [Vercel project configuration](https://vercel.com/docs/project-configuration) in [`vercel.json`](vercel.json), and available at [fastenv.bws.bio](https://fastenv.bws.bio) and [fastenv.vercel.app](https://fastenv.vercel.app).

**The version of `mkdocs-material` installed on Vercel is independent of the version listed in _pyproject.toml_. If the version of `mkdocs-material` is updated in _pyproject.toml_, it must also be updated in the Vercel build configuration.**
